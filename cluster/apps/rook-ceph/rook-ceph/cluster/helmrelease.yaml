---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
spec:
  interval: 15m
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.17.3
      sourceRef:
        kind: HelmRepository
        name: rook-ceph-charts
        namespace: flux-system
      interval: 15m
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  values:
    operatorNamespace: rook-ceph
    toolbox:
      enabled: true
      resources:
        requests:
          cpu: 15m
          memory: 105Mi
        limits:
          memory: 105Mi
    monitoring:
      enabled: true
      createPrometheusRules: true
    pspEnable: false
    cephClusterSpec:
      dataDirHostPath: /var/lib/rook
      skipUpgradeChecks: false
      continueUpgradeAfterChecksEvenIfNotHealthy: false
      waitTimeoutForHealthyOSDInMinutes: 10
      mon:
        count: 3
        allowMultiplePerNode: false
      mgr:
        count: 1
        modules:
          - name: pg_autoscaler
            enabled: true
      dashboard:
        enabled: true
        ssl: false
      crashCollector:
        disable: false
      cleanupPolicy:
        confirmation: ""
        sanitizeDisks:
          method: quick
          dataSource: zero
          iteration: 1
        allowUninstallWithVolumes: false
      removeOSDsIfOutAndSafeToRemove: false
      priorityClassNames:
        mon: system-node-critical
        osd: system-node-critical
        mgr: system-cluster-critical
      storage:
        useAllNodes: false
        useAllDevices: false
        config:
          osdsPerDevice: "1"
        nodes:
          - name: kmaster1
            devices:
              - name: "/dev/disk/by-id/ata-V-GEN09SM21AR128SDK_128GB_VGAR2021092700008074"
          - name: kmaster2
            devices:
              - name: "/dev/disk/by-id/ata-ADATA_SU650_2L3429SNKHFW"
          - name: kmaster3
            devices:
              - name: "/dev/disk/by-id/ata-V-GEN09SM21AR128SDK_128GB_VGAR2021092700008661"
      disruptionManagement:
        managePodBudgets: true
        osdMaintenanceTimeout: 30
        pgHealthCheckTimeout: 0
        manageMachineDisruptionBudgets: false
        machineDisruptionBudgetNamespace: openshift-machine-api
      healthCheck:
        daemonHealth:
          mon:
            disabled: false
            interval: 45s
          osd:
            disabled: false
            interval: 60s
          status:
            disabled: false
            interval: 60s
        livenessProbe:
          mon:
            disabled: false
          mgr:
            disabled: false
          osd:
            disabled: false
        startupProbe:
          mon:
            disabled: false
          mgr:
            disabled: false
          osd:
            disabled: false
      resources: null
    cephBlockPools:
      - name: blockpool
        spec:
          failureDomain: host
          replicated:
            size: 3
        storageClass:
          enabled: true
          name: rook-ceph-block
          isDefault: true
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          mountOptions: []
          parameters:
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4
    cephFileSystems: null
    cephObjectStores: null
